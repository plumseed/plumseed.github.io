<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/title_favicon_32.ico?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/title_favicon_32.ico?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/title_favicon_16.ico?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">
  <meta name="google-site-verification" content="3dBwV8OlVnNtYzxCLCFp2w8WMpuSecV7vBmA_zrf9j4">
  <meta name="baidu-site-verification" content="eoUZD1BDx6">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":"default"},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="TensorRT(TRT) 作为一种能显著加快深度学习模型 inference 的工具，如果能够较好的利用，可以显著提高我们的 GPU 使用效率和模型运行速度。">
<meta name="keywords" content="cuda,c++,TensorRT,trt,tensorflow,caffe">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT 实战教程">
<meta property="og:url" content="https://murphypei.github.io/blog/2019/09/trt-useage.html">
<meta property="og:site_name" content="拾荒志">
<meta property="og:description" content="TensorRT(TRT) 作为一种能显著加快深度学习模型 inference 的工具，如果能够较好的利用，可以显著提高我们的 GPU 使用效率和模型运行速度。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-09-06T09:24:43.030Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorRT 实战教程">
<meta name="twitter:description" content="TensorRT(TRT) 作为一种能显著加快深度学习模型 inference 的工具，如果能够较好的利用，可以显著提高我们的 GPU 使用效率和模型运行速度。">
  <link rel="alternate" href="/atom.xml" title="拾荒志" type="application/atom+xml">
  <link rel="canonical" href="https://murphypei.github.io/blog/2019/09/trt-useage">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>TensorRT 实战教程 | 拾荒志</title>
  <meta name="generator" content="Hexo 3.9.0">
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">拾荒志</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">虚怀若谷，大智若愚</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-about">
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
  </ul>

    

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://murphypei.github.io/blog/2019/09/trt-useage.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="murphypei">
      <meta itemprop="description" content="虚怀若谷，大智若愚">
      <meta itemprop="image" content="/images/avatar_128.ico">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="拾荒志">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">TensorRT 实战教程

          
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2019-09-09 15:39:03" itemprop="dateCreated datePublished" datetime="2019-09-09T15:39:03+08:00">2019-09-09</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-06 17:24:43" itemprop="dateModified" datetime="2021-09-06T17:24:43+08:00">2021-09-06</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          
          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
              
                <span class="post-meta-item-text">本文字数：</span>
              
              <span>7.8k</span>
            </span>
          
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
              
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              
              <span>13 分钟</span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>TensorRT(TRT) 作为一种能显著加快深度学习模型 inference 的工具，如果能够较好的利用，可以显著提高我们的 GPU 使用效率和模型运行速度。</p>
<a id="more"></a>
<p>TensorRT(TRT) 作为一种快速的 GPU 推理框架，其常规流程就是利用现有的模型文件编译一个 engine，在编译 engine 的过程中，会为每一层的计算操作找寻最优的算子方法，这样编译好的 engine 执行起来就非常高效。很类似 C++ 编译过程。</p>
<p>关于 TRT 的相关资料，我觉得还是以 NV 官方的为准。对于 TRT 而言，其在推理上的速度优势肯定是不言而喻的，它有多好，有多快，适不适合你的业务场景，这个需要大家自行判断。而其大概流程和一些基本优化，NV 官方的资料也做出了说明，可以参考这票文章 <a href="https://devblogs.nvidia.com/deploying-deep-learning-nvidia-tensorrt/" target="_blank" rel="noopener">deploying-deep-learning-nvidia-tensorrt</a> 。想快速了解 TRT 以及安装过程，也可以参考 <a href="https://arleyzhang.github.io/articles/7f4b25ce/" target="_blank" rel="noopener">TensorRT-介绍-使用-安装</a></p>
<p>本文根据自身实际使用过程中的记录而来，使用了 TRT 的 C++ 接口，更加注重编码的流程，配合讲解 TRT 的一些知识点，作为使用的总结。</p>
<h3 id="构建模型-和-engine"><a href="#构建模型-和-engine" class="headerlink" title="构建模型 和 engine"></a>构建模型 和 engine</h3><p>TRT 将模型结构和参数以及相应 kernel 计算方法都编译成一个二进制 engine，因此在部署之后大大加快了推理速度。为了能够使用 TRT 进行推理，需要创建一个 eninge。TRT 中 engine 的创建有两种方式：</p>
<ul>
<li>通过网络模型结构和参数文件编译得到，很慢。</li>
<li>读取一个已有的 engine（gie 文件），因为跳过了模型解析等过程，速度更快。</li>
</ul>
<p>第一种方式很慢，但是在第一次部署某个模型，或者修改模型的精度、输入数据类型、网络结构等等，只要修改了模型，就必须重新编译（其实 TRT 还有一种可以重新加载参数的方式，不是本文所涉及的）。</p>
<p>现在假设我们是第一次用 TRT，所以就只能选择第一种方式来创建一个 engine。为了创建一个 engine，我们需要有模型结构和模型参数两个文件，同时需要能够解析这两个文件的方法。在 TRT 中，编译 engine 是通过 <code>IBuilder</code> 对象进行的，因此我们首先需要新键一个 <code>IBuilder</code> 对象：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::IBuilder *builder = createInferBuilder(gLogger);</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>gLogger</code> 是 TRT 中的日志接口 <code>ILogger</code> ，继承这个接口并创建自己的 logger 对象传入即可。</p>
</blockquote>
<p>为了编译一个 engine，<code>builder</code> 首先需要创建一个 <code>INetworkDefinition</code> 作为模型的容器：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::INetworkDefinition *network = builder-&gt;createNetwork();</span><br></pre></td></tr></table></figure>
<p>注意，<strong>此时 <code>network</code> 是空的</strong>，我们需要填充模型结构和参数，也就是解析我们自己的模型结构和参数文件，获取数据放到其中。</p>
<p>TRT 官方给了三种主流框架模型格式的解析器（parser），分别是：</p>
<ul>
<li>ONNX：<code>IOnnxParser parser = nvonnxparser::createParser(*network, gLogger);</code></li>
<li>Caffe：<code>ICaffeParser parser = nvcaffeparser1::createCaffeParser();</code></li>
<li>UFF：<code>IUffParser parser = nvuffparser::createUffParser();</code></li>
</ul>
<p>其中 UFF 是用于 TensorFlow 的格式。调用这三种解析器就可以解析相应的文件。以 <code>ICaffeParser</code> 为例，调用其 <code>parse</code> 方法来填充 <code>network</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">virtual</span> <span class="keyword">const</span> IBlobNameToTensor* nvcaffeparser1::ICaffeParser::parse(</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span>* deploy, </span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">char</span> * model, </span><br><span class="line">	nvinfer1::INetworkDefinition &amp;network, </span><br><span class="line">	nvinfer1::DataType weightType)</span><br><span class="line"></span><br><span class="line"><span class="comment">//Parameters</span></span><br><span class="line"><span class="comment">//deploy	    The plain text, prototxt file used to define the network configuration.</span></span><br><span class="line"><span class="comment">//model	        The binaryproto Caffe model that contains the weights associated with the network.</span></span><br><span class="line"><span class="comment">//network	    Network in which the CaffeParser will fill the layers.</span></span><br><span class="line"><span class="comment">//weightType    The type to which the weights will transformed.</span></span><br></pre></td></tr></table></figure>
<p>这样就能得到一个填充好的 <code>network</code> ，就可以编译 engine 了，似乎一切都很美妙呢…</p>
<p>然而实际 TRT 并不完善，比如 TensorFlow 的很多操作并不支持，因此你传入的文件往往是根本就解析不了（深度学习框架最常见的困境之一）。因此我们需要自己去做填充 <code>network</code> 这件事，这就需要调用 TRT 中低级别的接口来创建模型结构，类似于你在 Caffe 或者 TensorFlow 中做的那样。</p>
<p>TRT 提供了较为丰富的接口让你可以直接通过这些接口创建自己的网络，比如添加一个卷积层：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">virtual</span> IConvolutionLayer* nvinfer1::INetworkDefinition::addConvolution(ITensor &amp;input, </span><br><span class="line">                                                                        <span class="keyword">int</span> nbOutputMaps,</span><br><span class="line">                                                                        DimsHW kernelSize,</span><br><span class="line">                                                                        Weights kernelWeights,</span><br><span class="line">                                                                        Weights biasWeights)		</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parameters</span></span><br><span class="line"><span class="comment">// input	The input tensor to the convolution.</span></span><br><span class="line"><span class="comment">// nbOutputMaps	The number of output feature maps for the convolution.</span></span><br><span class="line"><span class="comment">// kernelSize	The HW-dimensions of the convolution kernel.</span></span><br><span class="line"><span class="comment">// kernelWeights	The kernel weights for the convolution.</span></span><br><span class="line"><span class="comment">// biasWeights	The optional bias weights for the convolution.</span></span><br></pre></td></tr></table></figure>
<p>这里的参数基本上就是和其他深度学习框架类似的意思，没有什么好讲的。就是把数据封装成 TRT 中的数据结构即可。可能和平时构建训练网络不同的地方就是需要填充好模型的参数，因为 TRT 是推理框架，参数是已知确定的。这个过程一般是读取已经训练好的模型，构造 TRT 的数据结构类型放到其中，也就是需要你自己去解析模型参数文件。</p>
<p>之所以说 TRT 的网络构造接口是<strong>较为丰富</strong>，是因为即使使用这些低级接口这样，很多操作还是没办法完成，也就是没有相应的 <code>add*</code> 方法，更何况现实业务可能还会涉及很多自定义的功能层，因此 TRT 又有了 plugin 接口，允许你自己定义一个 <code>add*</code> 的操作。其流程就是继承 <code>nvinfer1::IPluginV2</code> 接口，利用 cuda 编写一个自定义层的功能，然后继承 <code>nvinfer1::IPluginCreator</code> 编写其创建类，需要重写其虚方法 <code>createPlugin</code>。最后调用 <code>REGISTER_TENSORRT_PLUGIN</code> 宏来注册这个 plugin 就可以用了。plugin 接口的成员函数介绍。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获得该自定义层的输出个数，比如 leaky relu 层的输出个数为1</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">getNbOutputs</span><span class="params">()</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 得到输出 Tensor 的维数</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> Dims <span class="title">getOutputDimensions</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">const</span> Dims* inputs, <span class="keyword">int</span> nbInputDims)</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置该层的参数。该函数在 initialize() 函数之前被构造器调用。它为该层提供了一个机会，可以根据其权重、尺寸和最大批量大小来做出算法选择。</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(<span class="keyword">const</span> Dims* inputDims, <span class="keyword">int</span> nbInputs, <span class="keyword">const</span> Dims* outputDims, <span class="keyword">int</span> nbOutputs, <span class="keyword">int</span> maxBatchSize)</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对该层进行初始化，在 engine 创建时被调用。</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">initialize</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 该函数在 engine 被摧毁时被调用</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">terminate</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获得该层所需的临时显存大小。</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> size_t <span class="title">getWorkspaceSize</span><span class="params">(<span class="keyword">int</span> maxBatchSize)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行该层</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">enqueue</span><span class="params">(<span class="keyword">int</span> batchSize, <span class="keyword">const</span> <span class="keyword">void</span>*<span class="keyword">const</span> * inputs, <span class="keyword">void</span>** outputs, <span class="keyword">void</span>* workspace, cudaStream_t stream)</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获得该层进行 serialization 操作所需要的内存大小</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> size_t <span class="title">getSerializationSize</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 序列化该层，根据序列化大小 getSerializationSize()，将该类的参数和额外内存空间全都写入到系列化buffer中。</span></span><br><span class="line"><span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(<span class="keyword">void</span>* buffer)</span> </span>= <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<p>我们需要根据自己层的功能，重写这里全部或者部分函数的实现，这里有很多细节，没办法一一展开，需要自定义的时候还是需要看官方 API。</p>
<p>构建好了网络模型，就可以执行 engine 的编译了，还需要对 engine 进行一些设置。比如计算精度，支持的 batch size 等等，因为这些设置不同，编译出来的 engine 也不同。</p>
<p>TRT 支持 FP16 计算，也是官方推荐的计算精度，其设置也比简单，直接调用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">builder-&gt;setFp16Mode(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>
<p>另外在设置精度的时候，还有一个设置 strict 策略的接口：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">builder-&gt;setStrictTypeConstraints(<span class="literal">true</span>);</span><br></pre></td></tr></table></figure>
<p>这个接口就是是否严格按照设置的精度进行类型转换，如果不设置 strict 策略，则 TRT 在某些计算中可能会选择更高精度（不影响性能）的计算类型。 </p>
<p>除了精度，还需要设置好运行的 batch size 和 workspace size：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">builder-&gt;setMaxBatchSize(batch_size);</span><br><span class="line">builder-&gt;setMaxWorkspaceSize(workspace_size);</span><br></pre></td></tr></table></figure>
<p>这里的 batch size 是运行时最大能够支持的 batch size，运行时可以选择比这个值小的 batch size，workspace 也是相对于这个最大 batch size 设置的。</p>
<p>设置好上述参数，就可以编译 engine 了。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::ICudaEngine *engine = builder-&gt;buildCudaEngine(*network);</span><br></pre></td></tr></table></figure>
<p>编译需要花较长时间，耐心等待。</p>
<h3 id="Engine-序列化和反序列化"><a href="#Engine-序列化和反序列化" class="headerlink" title="Engine 序列化和反序列化"></a>Engine 序列化和反序列化</h3><p>编译 engine 需要较长时间，在模型和计算精度、batch size 等均保持不变的情况下，我们可以选择保存 engine 到本地，供下次运行使用，也就是 engine 序列化。TRT 提供了很方便的序列化方法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvinfer1::IHostMemory *modelStream = engine-&gt;serialize();</span><br></pre></td></tr></table></figure>
<p>通过这个调用，得到的是一个二进制流，将这个流写入到一个文件中即可保存下来。</p>
<p>如果需要再次部署，可以直接反序列化保存好的文件，略过编译环节。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IRuntime* runtime = createInferRuntime(gLogger);</span><br><span class="line">ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(modelData, modelSize, <span class="literal">nullptr</span>);</span><br></pre></td></tr></table></figure>
<h3 id="使用-engine-进行预测"><a href="#使用-engine-进行预测" class="headerlink" title="使用 engine 进行预测"></a>使用 engine 进行预测</h3><p>有了 engine 之后就可以使用它进行 inference 了。</p>
<p>首先创建一个 inference 的 context。这个 context 类似命名空间，用于保存一个 inference 任务的变量。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IExecutionContext *context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>
<p><strong>一个 engine 可以有多个 context</strong>，也就是说一个 engine 可以同时进行多个预测任务。</p>
<p>然后就是绑定输入和输出的 index。这一步的原因在于 TRT 在 build engine 的过程中，将输入和输出映射为索引编号序列，因此我们只能通过索引编号来获取输入和输出层的信息。虽然 TRT 提供了通过名字获取索引编号的接口，但是本地保存可以方便后续操作。</p>
<p>我们可以先获取索引编号的数量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> index_number = engine-&gt;getNbBindings();</span><br></pre></td></tr></table></figure>
<p>我们可以判断这个编号数量是不是和我们网络的输入输出之和相同，比如你有一个输入和一个输出，那么编号的数量就是2。如果不是，则说明这个 engine 是有问题的；如果没问题，我们就可以通过名字获取输入输出对应的序号：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> input_index = engine-&gt;getBindingIndex(input_layer_name);</span><br><span class="line"><span class="keyword">int</span> output_index = engine-&gt;getBindingIndex(output_layer_name);</span><br></pre></td></tr></table></figure>
<p>对于常见的一个输入和输出的网络，输入的索引编号就是 0，输出的索引编号就是 1，所以这一步也不是必须的。</p>
<p>接下来就需要为输入和输出层分配显存空间了。为了分配显存空间，我们需要知道输入输出的维度信息和存放的数据类型，TRT 中维度信息和数据类型的表示如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dims</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> MAX_DIMS = <span class="number">8</span>; <span class="comment">//!&lt; The maximum number of dimensions supported for a tensor.</span></span><br><span class="line">    <span class="keyword">int</span> nbDims;                    <span class="comment">//!&lt; The number of dimensions.</span></span><br><span class="line">    <span class="keyword">int</span> d[MAX_DIMS];               <span class="comment">//!&lt; The extent of each dimension.</span></span><br><span class="line">    DimensionType type[MAX_DIMS];  <span class="comment">//!&lt; The type of each dimension.</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">enum</span> <span class="class"><span class="keyword">class</span> <span class="title">DataType</span> :</span> <span class="keyword">int</span></span><br><span class="line">&#123;</span><br><span class="line">    kFLOAT = <span class="number">0</span>, <span class="comment">//!&lt; FP32 format.</span></span><br><span class="line">    kHALF = <span class="number">1</span>,  <span class="comment">//!&lt; FP16 format.</span></span><br><span class="line">    kINT8 = <span class="number">2</span>,  <span class="comment">//!&lt; quantized INT8 format.</span></span><br><span class="line">    kINT32 = <span class="number">3</span>  <span class="comment">//!&lt; INT32 format.</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>我们通过索引编号获取输入和输出的数据维度（dims）和数据类型（dtype），然后为每个输出层开辟显存空间，存放输出结果：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; index_number; ++i)</span><br><span class="line">&#123;</span><br><span class="line">	nvinfer1::Dims dims = engine-&gt;getBindingDimensions(i);</span><br><span class="line">	nvinfer1::DataType dtype = engine-&gt;getBindingDataType(i);</span><br><span class="line">    <span class="comment">// 获取数据长度</span></span><br><span class="line">    <span class="keyword">auto</span> buff_len = <span class="built_in">std</span>::accumulate(dims.d, dims.d + dims.nbDims, <span class="number">1</span>, <span class="built_in">std</span>::multiplies&lt;<span class="keyword">int64_t</span>&gt;());</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// 获取数据类型大小</span></span><br><span class="line">    dtype_size = getTypeSize(dtype);	<span class="comment">// 自定义函数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为 output 分配显存空间</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> &amp;output_i : outputs)</span><br><span class="line">&#123;</span><br><span class="line">    cudaMalloc(buffer_len_i * dtype_size_i * batch_size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本文给出的是伪代码，仅表示逻辑，因此会涉及一些简单的自定义函数。</p>
</blockquote>
<p>至此，我们已经做好了准备工作，现在就可以把数据塞进模型进行推理了。</p>
<h4 id="前向预测"><a href="#前向预测" class="headerlink" title="前向预测"></a>前向预测</h4><p>TRT 的前向预测执行是异步的，context 通过一个 enqueue 调用来提交任务：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream;</span><br><span class="line">cudaStreamCreate(&amp;stream);</span><br><span class="line">context-&gt;enqueue(batch_size, buffers, stream, <span class="literal">nullptr</span>);</span><br><span class="line">cudaStreamSynchronize(stream);</span><br></pre></td></tr></table></figure>
<p>enqueue 是 TRT 的实际执行任务的函数，我们在写 plugin 的时候也需要实现这个函数接口。其中：</p>
<ul>
<li><p><code>batch_size</code>：engine 在 build 过程中传入的 <code>max_batch_size</code>。</p>
</li>
<li><p><code>buffers</code>：是一个指针数组，其下标对应的就是输入输出层的索引编号，存放的就是输入的数据指针以及输出的数据存放地址（也就是开辟的显存地址）。</p>
</li>
<li><p><code>stream</code>：stream 是 cuda 一系列顺序操作的概念。对于我们的模型来说就是将所有的模型操作按照（网络结构）指定的顺序在指定的设备上执行。</p>
<blockquote>
<p>cuda stream 是指一堆异步的 cuda 操作，他们按照 host 代码调用的顺序执行在 device 上。stream 维护了这些操作的顺序，并在所有预处理完成后允许这些操作进入工作队列，同时也可以对这些操作进行一些查询操作。这些操作包括 host 到 device 的数据传输，launch kernel 以及其他的 host 发起由 device 执行的动作。这些操作的执行总是异步的，cuda runtime 会决定这些操作合适的执行时机。我们则可以使用相应的cuda api 来保证所取得结果是在所有操作完成后获得的。<strong>同一个 stream 里的操作有严格的执行顺序</strong>，不同的 stream 则没有此限制。</p>
</blockquote>
</li>
</ul>
<p>这里需要注意，输入数据和输出数据在 buffers 数组中都是在 GPU 上的，可以通过 <code>cudaMemcpy</code> 拷贝 CPU 上的输入数据到 GPU 中（需要提前开辟一块显存来存放）。同理，输出数据也需要从 GPU 中拷贝到 CPU 中。</p>
<p>前两句创建了一个 cuda stream，最后一句则是等待这个异步 stream 执行完毕，然后从显存中将数据拷贝出来即可。</p>
<p>至此，我们就完成了 TRT 一个基本的预测流程。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文仅仅是针对 TRT 的预测流程和一些常见调用进行了说明，并不涉及具体网络和具体实现，也没有太多编码的细节。不同网络不同操作需要一些扩展 plugin 的编写，而对于编码，包括内存和显存的开辟管理，TRT 的析构清理工作等等都不在本文叙述范围之内。</p>
<h4 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h4><ul>
<li><a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#c_topics" target="_blank" rel="noopener">https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#c_topics</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/index.html" target="_blank" rel="noopener">https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/index.html</a></li>
<li><a href="https://www.cnblogs.com/1024incn/p/5891051.html" target="_blank" rel="noopener">https://www.cnblogs.com/1024incn/p/5891051.html</a></li>
</ul>

    </div>

    
    
    
        
      
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>murphypei</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://murphypei.github.io/blog/2019/09/trt-useage.html" title="TensorRT 实战教程">https://murphypei.github.io/blog/2019/09/trt-useage.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>
</div>

      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/cuda/" rel="tag"># cuda</a>
            
              <a href="/tags/c/" rel="tag"># c++</a>
            
              <a href="/tags/TensorRT/" rel="tag"># TensorRT</a>
            
              <a href="/tags/trt/" rel="tag"># trt</a>
            
              <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
            
              <a href="/tags/caffe/" rel="tag"># caffe</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/blog/2019/08/vm-black-screen.html" rel="next" title="win10 虚拟机黑屏卡死">
                  <i class="fa fa-chevron-left"></i> win10 虚拟机黑屏卡死
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/blog/2019/09/python-logging.html" rel="prev" title="Python 日志库 logging 总结">
                  Python 日志库 logging 总结 <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#构建模型-和-engine"><span class="nav-number">1.</span> <span class="nav-text">构建模型 和 engine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Engine-序列化和反序列化"><span class="nav-number">2.</span> <span class="nav-text">Engine 序列化和反序列化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用-engine-进行预测"><span class="nav-number">3.</span> <span class="nav-text">使用 engine 进行预测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前向预测"><span class="nav-number">3.1.</span> <span class="nav-text">前向预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#参考资料："><span class="nav-number">4.1.</span> <span class="nav-text">参考资料：</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar_128.ico"
      alt="murphypei">
  <p class="site-author-name" itemprop="name">murphypei</p>
  <div class="site-description" itemprop="description">虚怀若谷，大智若愚</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">173</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">444</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/murphypei" title="GitHub &rarr; https://github.com/murphypei" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:murphypei47@gmail.com" title="E-Mail &rarr; mailto:murphypei47@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/guo-jia-66-80/activities" title="https://www.zhihu.com/people/guo-jia-66-80/activities" rel="noopener" target="_blank">知乎</a>
        </li>
      
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">murphypei</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">605k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">16:49</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
<script src="/js/utils.js?v=7.3.0"></script><script src="/js/motion.js?v=7.3.0"></script>
<script src="/js/schemes/muse.js?v=7.3.0"></script>

<script src="/js/next-boot.js?v=7.3.0"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>





















  

  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: 'e14928c5d4e586a1be33',
      clientSecret: 'b58488475e69824177de7fa4e52325a0de1dbdb7',
      repo: 'murphypei.github.io',
      owner: 'murphypei',
      admin: ['murphypei'],
      id: '2b61dbbae89cf510ecf9939e404c0cff',
        language: 'zh-CN',
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/haru01.model.json"},"display":{"position":"left","width":250,"height":400},"mobile":{"show":false}});</script></body>
</html>
